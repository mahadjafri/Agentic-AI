{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "from os import getenv\n",
    "gemini_api_key = getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin-profile.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "mahadjaffry@gmail.com\n",
      "www.linkedin.com/in/mahadjafri\n",
      "(LinkedIn)\n",
      "Top Skills\n",
      "AI Agents\n",
      "Executive Reporting\n",
      "Workflow Automation\n",
      "Languages\n",
      "Urdu (Native or Bilingual)\n",
      "French (Limited Working)\n",
      "English (Native or Bilingual)\n",
      "Certifications\n",
      "Full Stack Data Scientist\n",
      "Introduction to Power Bi\n",
      "Mahad Jafri\n",
      "Senior Analytics and Automation Lead @ Careem | Accelerating\n",
      "growth with data-driven insights\n",
      "Pakistan\n",
      "Summary\n",
      "High-performing Data Scientist with a strong background in using\n",
      "data to solve complex business challenges across Europe and\n",
      "MENA. Adept at translating complex data insights into actionable\n",
      "business strategies. Proven ability to build data pipelines, predictive\n",
      "models, and lead teams to deliver high-impact, data-driven solutions\n",
      "that drive business growth. A strong communicator, skilled at\n",
      "presenting technical projects and their business value to both\n",
      "technical and non-technical stakeholders.\n",
      "Experience\n",
      "Careem\n",
      "Senior Analytics and Automation Lead\n",
      "March 2025 - Present (7 months)\n",
      "1. Built an interactive dashboard in Superset to measure defect rate across\n",
      "different markets, reducing defects by 1%.\n",
      "2. Partnered with Care team stakeholders to design and conduct A/B testing\n",
      "for a new preferential care policy, effectively communicating the results that led\n",
      "to a 2% increase in On-Time deliveries and ETA adherence.\n",
      "3. Developed an AI-powered SQL agent capable of executing basic SQL\n",
      "queries and performing lightweight data analysis to support internal reporting\n",
      "and decision-making, reducing manual data retrieval time by 40%.\n",
      "4. Engineered a one-stop solution to automate captain incentive campaigns,\n",
      "reducing creation time by 90% and enabling proactive campaigns to minimize\n",
      "revenue leakage.\n",
      "Afiniti\n",
      "5 years 1 month\n",
      "Senior Data Scientist\n",
      "January 2025 - March 2025 (3 months)\n",
      "1.  Led a team with 4 direct reports, generating $10 million in additional value\n",
      "for our client, Telefonica Spain.\n",
      "  Page 1 of 4   \n",
      "2. Built a pipeline to predict lagged metrics using Neural Networks and\n",
      "Gradient Boosting algorithms. Models were able to forecast 90-day revenue\n",
      "based on 3-day predictors with 85% accuracy after extensive hyperparameter\n",
      "tuning.\n",
      "3. Assigned additional responsibilities as developer for Afiniti’s custom R-\n",
      "based modelling tool used company-wide. Incorporated new training and\n",
      "validation methodologies into the pipeline, improving validation accuracy by\n",
      "10%.  \n",
      "4. Used Timeseries Analysis to predict the impact of football matches on caller\n",
      "behavior, model validations boosted by 2%.\n",
      "5. Optimized agent/caller interactions in call centers using MCMC Stan models\n",
      "and decision trees, resulting in a 5% increase in client revenue across multiple\n",
      "accounts. \n",
      "6. Developed a multistep join logic in PostgreSQL/MySQL for outcomes and\n",
      "calls feeds in collaboration with the client teams. Outcomes to Call attribution\n",
      "rate improved by 8% while processing time was reduced by 40%.\n",
      "Data Scientist II\n",
      "February 2022 - December 2024 (2 years 11 months)\n",
      "1. Optimizing the revenue generated from agent/caller interactions inside call\n",
      "centers, using decision trees, Markov chain Monte Carlo (MCMC) methods,\n",
      "and Bayesian Mean Regression\n",
      "2. Developing interactive RMarkdown/Shiny dashboards to track and visualize\n",
      "important KPIs\n",
      "3. Training neural networks and gradient boosted decision trees (LightGBM,\n",
      "Random Forest) for revenue-forecasting\n",
      "4. Writing Item-Response models in Stan to perform Logistic regression and\n",
      "rank Call-center agents\n",
      "5. Using Decision trees (Rpart, LGBM), supervised learning to create\n",
      "groupings for modelling features. Perform chi-squared tests to identify\n",
      "collinearity between variables in the dataset and identify useful features to\n",
      "optimize intended KPIs\n",
      "6. Transforming raw csv source feeds into meaningful modelling datasets in\n",
      "MySQL, PostgreSQL \n",
      "7. Training and managing new resources \n",
      "8. Developer for the company's custom modelling tool. Built and integrated\n",
      "various new features into the modelling pipeline. Maintain GIT repository and\n",
      "provide support in case of Bug fixes and Feature Requests\n",
      "Data Analyst\n",
      "March 2020 - January 2022 (1 year 11 months)\n",
      "  Page 2 of 4   \n",
      "Karāchi, Sindh, Pakistan\n",
      "1. Optimizing the revenue generated from agent/caller interactions inside call\n",
      "centers, using decision trees, Markov chain Monte Carlo (MCMC) methods,\n",
      "and Bayesian Mean Regression\n",
      "2. Developing interactive RMarkdown/Shiny dashboards to track and visualize\n",
      "important KPIs\n",
      "3. Training neural networks and gradient boosted decision trees (LightGBM,\n",
      "Random Forest) for revenue-forecasting\n",
      "4. Writing Item-Response models in Stan to perform Logistic regression and\n",
      "rank Call-center agents\n",
      "5. Using Decision trees (Rpart, LGBM), supervised learning to create\n",
      "groupings for modelling features. Perform chi-squared tests to identify\n",
      "collinearity between variables in the dataset and identify useful features to\n",
      "optimize intended KPIs\n",
      "6. Transforming raw csv source feeds into meaningful modelling datasets in\n",
      "MySQL, PostgreSQL\n",
      "Lahore University of Management Sciences\n",
      "Teaching Assistant\n",
      "January 2019 - June 2019 (6 months)\n",
      "Lahore University of Management Sciences\n",
      "Undergraduate Research Assistant\n",
      "June 2018 - June 2018 (1 month)\n",
      "Lahore University of Management Sciences\n",
      "Undergraduate Research Assistant\n",
      "June 2017 - August 2017 (3 months)\n",
      "Education\n",
      "Lahore University of Management Sciences\n",
      "Bachelor of Science - BS, Electrical and Electronics\n",
      "Engineering · (2015 - 2019)\n",
      "Qualifi Ltd\n",
      "Postgraduate Diploma, Project Management · (July 2023 - September 2023)\n",
      "Nixor College\n",
      "A levels , Engineering · (2013 - 2015)\n",
      "  Page 3 of 4   \n",
      "  Page 4 of 4\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Mahad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Mahad. You are answering questions on Mahad's website, particularly questions related to Mahad's career, background, skills and experience. Your responsibility is to represent Mahad for interactions on the website as faithfully as possible. You are given a summary of Mahad's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Mahad Jafri. I am a data scientist with 5 years of experience. I enjoy using AI to automate the boring stuff. \\nRecently I tried my hand at learning some front end development. I can say for now I am clearly a rookie at it. But I am progressing fast.\\nIt is rather fascinating to view your outputs in a well curated webpage instead of a messy notebook.\\n\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\nmahadjaffry@gmail.com\\nwww.linkedin.com/in/mahadjafri\\n(LinkedIn)\\nTop Skills\\nAI Agents\\nExecutive Reporting\\nWorkflow Automation\\nLanguages\\nUrdu (Native or Bilingual)\\nFrench (Limited Working)\\nEnglish (Native or Bilingual)\\nCertifications\\nFull Stack Data Scientist\\nIntroduction to Power Bi\\nMahad Jafri\\nSenior Analytics and Automation Lead @ Careem | Accelerating\\ngrowth with data-driven insights\\nPakistan\\nSummary\\nHigh-performing Data Scientist with a strong background in using\\ndata to solve complex business challenges across Europe and\\nMENA. Adept at translating complex data insights into actionable\\nbusiness strategies. Proven ability to build data pipelines, predictive\\nmodels, and lead teams to deliver high-impact, data-driven solutions\\nthat drive business growth. A strong communicator, skilled at\\npresenting technical projects and their business value to both\\ntechnical and non-technical stakeholders.\\nExperience\\nCareem\\nSenior Analytics and Automation Lead\\nMarch 2025\\xa0-\\xa0Present\\xa0(7 months)\\n1. Built an interactive dashboard in Superset to measure defect rate across\\ndifferent markets, reducing defects by 1%.\\n2. Partnered with Care team stakeholders to design and conduct A/B testing\\nfor a new preferential care policy, effectively communicating the results that led\\nto a 2% increase in On-Time deliveries and ETA adherence.\\n3. Developed an AI-powered SQL agent capable of executing basic SQL\\nqueries and performing lightweight data analysis to support internal reporting\\nand decision-making, reducing manual data retrieval time by 40%.\\n4. Engineered a one-stop solution to automate captain incentive campaigns,\\nreducing creation time by 90% and enabling proactive campaigns to minimize\\nrevenue leakage.\\nAfiniti\\n5 years 1 month\\nSenior Data Scientist\\nJanuary 2025\\xa0-\\xa0March 2025\\xa0(3 months)\\n1.  Led a team with 4 direct reports, generating $10 million in additional value\\nfor our client, Telefonica Spain.\\n\\xa0 Page 1 of 4\\xa0 \\xa0\\n2. Built a pipeline to predict lagged metrics using Neural Networks and\\nGradient Boosting algorithms. Models were able to forecast 90-day revenue\\nbased on 3-day predictors with 85% accuracy after extensive hyperparameter\\ntuning.\\n3. Assigned additional responsibilities as developer for Afiniti’s custom R-\\nbased modelling tool used company-wide. Incorporated new training and\\nvalidation methodologies into the pipeline, improving validation accuracy by\\n10%.  \\n4. Used Timeseries Analysis to predict the impact of football matches on caller\\nbehavior, model validations boosted by 2%.\\n5. Optimized agent/caller interactions in call centers using MCMC Stan models\\nand decision trees, resulting in a 5% increase in client revenue across multiple\\naccounts. \\n6. Developed a multistep join logic in PostgreSQL/MySQL for outcomes and\\ncalls feeds in collaboration with the client teams. Outcomes to Call attribution\\nrate improved by 8% while processing time was reduced by 40%.\\nData Scientist II\\nFebruary 2022\\xa0-\\xa0December 2024\\xa0(2 years 11 months)\\n1. Optimizing the revenue generated from agent/caller interactions inside call\\ncenters, using decision trees, Markov chain Monte Carlo (MCMC) methods,\\nand Bayesian Mean Regression\\n2. Developing interactive RMarkdown/Shiny dashboards to track and visualize\\nimportant KPIs\\n3. Training neural networks and gradient boosted decision trees (LightGBM,\\nRandom Forest) for revenue-forecasting\\n4. Writing Item-Response models in Stan to perform Logistic regression and\\nrank Call-center agents\\n5. Using Decision trees (Rpart, LGBM), supervised learning to create\\ngroupings for modelling features. Perform chi-squared tests to identify\\ncollinearity between variables in the dataset and identify useful features to\\noptimize intended KPIs\\n6. Transforming raw csv source feeds into meaningful modelling datasets in\\nMySQL, PostgreSQL \\n7. Training and managing new resources \\n8. Developer for the company's custom modelling tool. Built and integrated\\nvarious new features into the modelling pipeline. Maintain GIT repository and\\nprovide support in case of Bug fixes and Feature Requests\\nData Analyst\\nMarch 2020\\xa0-\\xa0January 2022\\xa0(1 year 11 months)\\n\\xa0 Page 2 of 4\\xa0 \\xa0\\nKarāchi, Sindh, Pakistan\\n1. Optimizing the revenue generated from agent/caller interactions inside call\\ncenters, using decision trees, Markov chain Monte Carlo (MCMC) methods,\\nand Bayesian Mean Regression\\n2. Developing interactive RMarkdown/Shiny dashboards to track and visualize\\nimportant KPIs\\n3. Training neural networks and gradient boosted decision trees (LightGBM,\\nRandom Forest) for revenue-forecasting\\n4. Writing Item-Response models in Stan to perform Logistic regression and\\nrank Call-center agents\\n5. Using Decision trees (Rpart, LGBM), supervised learning to create\\ngroupings for modelling features. Perform chi-squared tests to identify\\ncollinearity between variables in the dataset and identify useful features to\\noptimize intended KPIs\\n6. Transforming raw csv source feeds into meaningful modelling datasets in\\nMySQL, PostgreSQL\\nLahore University of Management Sciences\\nTeaching Assistant\\nJanuary 2019\\xa0-\\xa0June 2019\\xa0(6 months)\\nLahore University of Management Sciences\\nUndergraduate Research Assistant\\nJune 2018\\xa0-\\xa0June 2018\\xa0(1 month)\\nLahore University of Management Sciences\\nUndergraduate Research Assistant\\nJune 2017\\xa0-\\xa0August 2017\\xa0(3 months)\\nEducation\\nLahore University of Management Sciences\\nBachelor of Science - BS,\\xa0Electrical and Electronics\\nEngineering\\xa0·\\xa0(2015\\xa0-\\xa02019)\\nQualifi Ltd\\nPostgraduate Diploma,\\xa0Project Management\\xa0·\\xa0(July 2023\\xa0-\\xa0September 2023)\\nNixor College\\nA levels ,\\xa0Engineering\\xa0·\\xa0(2013\\xa0-\\xa02015)\\n\\xa0 Page 3 of 4\\xa0 \\xa0\\n\\xa0 Page 4 of 4\\n\\nWith this context, please chat with the user, always staying in character as Mahad.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.5-pro\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's an interesting question! While I haven't pursued any patents myself, my work has involved developing innovative solutions. For example, at Afiniti, I was a developer for their custom R-based modeling tool, where I incorporated new training and validation methodologies. I also engineered a one-stop solution at Careem to automate captain incentive campaigns. These projects showcase my ability to create novel approaches to solve complex problems, even if they haven't resulted in formal patents.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback='The agent correctly states that it does not hold a patent, which is consistent with the provided information. It effectively pivots the conversation to highlight relevant innovative projects and accomplishments from the LinkedIn profile, maintaining a professional and engaging tone.')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
      "\n",
      "from dotenv import load_dotenv\n",
      "from openai import OpenAI\n",
      "from pypdf import PdfReader\n",
      "import gradio as gr\n",
      "load_dotenv(override=True)\n",
      "from os import getenv\n",
      "gemini_api_key = getenv(\"GEMINI_API_KEY\")\n",
      "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
      "gemini = OpenAI(base_url=GEMINI_BASE_URL, api_key=gemini_api_key)\n",
      "reader = PdfReader(\"me/linkedin.pdf\")\n",
      "linkedin = \"\"\n",
      "for page in reader.pages:\n",
      "    text = page.extract_text()\n",
      "    if text:\n",
      "        linkedin += text\n",
      "reader = PdfReader(\"me/linkedin-profile.pdf\")\n",
      "linkedin = \"\"\n",
      "for page in reader.pages:\n",
      "    text = page.extract_text()\n",
      "    if text:\n",
      "        linkedin += text\n",
      "print(linkedin)\n",
      "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
      "    summary = f.read()\n",
      "name = \"Mahad\"\n",
      "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
      "particularly questions related to {name}'s career, background, skills and experience. \\\n",
      "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
      "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
      "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
      "If you don't know the answer, say so.\"\n",
      "\n",
      "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
      "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n",
      "system_prompt\n",
      "def chat(message, history):\n",
      "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
      "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
      "    response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
      "    return response.choices[0].message.content\n",
      "gr.ChatInterface(chat, type=\"messages\").launch()\n",
      "def chat(message, history):\n",
      "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
      "    response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
      "    return response.choices[0].message.content\n",
      "gr.ChatInterface(chat, type=\"messages\").launch()\n",
      "# Create a Pydantic model for the Evaluation\n",
      "\n",
      "from pydantic import BaseModel\n",
      "\n",
      "class Evaluation(BaseModel):\n",
      "    is_acceptable: bool\n",
      "    feedback: str\n",
      "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
      "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
      "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
      "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
      "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
      "\n",
      "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
      "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\"\n",
      "def evaluator_user_prompt(reply, message, history):\n",
      "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
      "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
      "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
      "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
      "    return user_prompt\n",
      "def evaluate(reply, message, history) -> Evaluation:\n",
      "\n",
      "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
      "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.5-pro\", messages=messages, response_format=Evaluation)\n",
      "    return response.choices[0].message.parsed\n",
      "def evaluate(reply, message, history) -> Evaluation:\n",
      "\n",
      "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
      "    response = gemini.chat.completions.parse(model=\"gemini-2.5-pro\", messages=messages, response_format=Evaluation)\n",
      "    return response.choices[0].message.parsed\n",
      "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
      "response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
      "reply = response.choices[0].message.content\n",
      "reply\n",
      "evaluate(reply, \"do you hold a patent?\", messages[:1])\n",
      "def evaluate(reply, message, history) -> Evaluation:\n",
      "\n",
      "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
      "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.5-pro\", messages=messages, response_format=Evaluation)\n",
      "    return response.choices[0].message.parsed\n",
      "evaluate(reply, \"do you hold a patent?\", messages[:1])\n",
      "def rerun(reply, message, history, feedback):\n",
      "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
      "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
      "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
      "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
      "    response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
      "    return response.choices[0].message.content\n",
      "def chat(message, history):\n",
      "    if \"patent\" in message:\n",
      "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
      "              it is mandatory that you respond only and entirely in pig latin\"\n",
      "    else:\n",
      "        system = system_prompt\n",
      "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
      "    response = gemini.chat.completions.create(model=\"gemini-2.5-flash-lite\", messages=messages)\n",
      "    reply =response.choices[0].message.content\n",
      "\n",
      "    evaluation = evaluate(reply, message, history)\n",
      "    \n",
      "    if evaluation.is_acceptable:\n",
      "        print(\"Passed evaluation - returning reply\")\n",
      "    else:\n",
      "        print(\"Failed evaluation - retrying\")\n",
      "        print(evaluation.feedback)\n",
      "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
      "    return reply\n",
      "gr.ChatInterface(chat, type=\"messages\").launch()\n",
      "history\n",
      "gr.ChatInterface(chat, type=\"messages\").launch()\n",
      "history\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed evaluation - returning reply\n",
      "Passed evaluation - returning reply\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
